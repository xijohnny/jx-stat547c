% !TEX root = ../main.tex

% introduction

\section{Introduction}

\paragraph{} Bayesian inference is a elegant framework for statistical analysis when the analyst wishes to embed probabilistic prior beliefs into a model. These prior beliefs are then updated according to the data to produce a posterior distribution of the parameters. For much of its history, Bayesian analysis was merely a clever idea that was hindered by the analytic tractability of complex high dimensional integrals in normalizing constant of the fitted posterior distribution. However more recent computational advancements have allowed for the increased applicability of the general Bayesian framework for highly expressive statistical modeling. Today, Bayesian methods are deployed in various scientific and machine learning disciplines as the state of the art, from clinical trials to text and image analysis. 

\paragraph{} One computational strategy that has gained popularity recently is known as variational inference (VI). VI is a member of a broader class of methods known as approximate inference. Instead of attempting to solve the normalizing integral, VI proposes a family of simpler distributions and finds a best approximation to the posterior, given the data and prior. In most cases, the variational family does not include the true posterior, and so it is truly an approximation, with no aim to recover exact posteriors. VI uses tools from optimization theory and has been proven to be faster than its counterparts and is often the only feasible strategy for extremely complex cases. However, the statistical propeties of VI outputs are not as well understood as it is often unable to directly leverage the elegance of Bayesian analysis due to the necessarily imperfect approximation. 

\paragraph{} The theoretical development of VI in the scope of classic statistical theory is hence a much needed task in the modern landscape. The popularity of VI suggests that it performs well in many relevant real-world scenarios, and that in turn suggests that theoretical properties are not necessarily absent, but rather simply underdeveloped. In a first step towards understanding the theoretical properties of VI, \cite{Wang:2019:VBVM} show a version of the classical Bernstein--von Mises Theorem for variational approximations to a parametric Bayesian posterior. Such a result can not only justify the current use of variational inference, but also opens the door to further theoretical developments, much like the impact of the original Bernstein--von Mises result. 

\paragraph{} The rest of the report will consist of three main sections. We will begin with a brief discussion of Bayesian asymptotics and hence the original Bernstein--von Mises Theorem, then a short introduction to VI and the mean-field family. Then, we will describe the chain of implications, written as four lemmata, that eventually leads to the variational Bernstein von--Mises result. Finally, we discuss the future research directions that this result inspires, and some specific open questions. We also provide two short exercises in the appendix, relevant to the understanding of the report. The actual proof of the results are highly technical, and so due to the expository nature of the work and limited space, will be described largely by proof sketches. 