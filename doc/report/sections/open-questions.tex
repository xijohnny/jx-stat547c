% !TEX root = ../main.tex

% open questions section

\section{Open questions and research directions}

This report was an exposition on the Variational Bernstein von--Mises theorem, as initially proven in \cite{Wang:2019:VBVM}. Almost concurrently, the same authors extended the result to misspecified models, i.e., the model $P_\Theta$ does not include $P_{\theta_0}$ \cite{Wang::mis2019}. The results here are appealing for a number of reasons. Bayesian statisticians have long used BvM-type results to justify the validity of their procedures asymptotically. Modern Bayesian inference is typically fit using Markov Chain Monte carlo (MCMC), which has nice theoretical properties in the limit, as samples from MCMC algorithms can be viewed as being from the exact posterior. The use of VI, which may never recover the exact posterior, is hence not as appealing for deriving theoretical guarantees. Nonetheless, for highly expressive models with large dimension and sample size, VI is much faster than MCMC and often the only feasible choice. Obtaining a theory for VI is a necessary precursor to obtain theoretical guarantees on these models, which have seen major applications in the real world. 

\paragraph{} The applicability of the results here are less obvious in the real world use-cases of VI. Specifically, it is unlikely that checking \ref{lem3} will be straight forward for most models that applied users are interested in. This is not a fault of the variational BvM, in fact the original BvM often does not hold for many popular models, including many non-parametric Bayesian models. Much research is still being done to extend the standard BvM to these cases, and in many cases entire articles are dedicated to establishing these asymptotics even in model specific cases (see for example, \cite{Castillo::2013} and \cite{Castillo::2015}, amongst many others). In this sense, the upcoming research directions are extremely vast for the variational BvM, and the open questions are plentiful.  

\paragraph{} A more focused research direction is one that is complementary to the recent theoretical developments of VI. \cite{GiordanoJMLR} examines the infinitesimal sensitivity of mean-field VB to changes in the model and prior, and it would be natural to consider the consequences for asymptotics as well. There has also been an interest in designing alternative objectives for VI beyond the standard KL divergence and ELBO dual. For example, the $\alpha$-divergence \cite{renyi2016} and $f$-divergence \cite{f2020} are progressively more general families that include the KL divergence. All the results in \cite{Wang:2019:VBVM} technically rests on the KL objective, but the main ideas, even within each lemma, extend to any variational objective. 

\paragraph{} The VI optimization is often approximate in practice, using techniques such as automatic differentiation and stochastic optimization. It would be an interesting research item to examine lemma \ref{lem3} under approximate solutions. The proof of this lemma already rests on the idea of $\Gamma$-convergence, which is a classical technique in optimization. It is not unreasonable to think that some results could be obtained in connection with modern developments in optimization. 

\paragraph{} We conclude that there are a number of future research directions not only for the asymptotic study of variational Bayes, but for the theoretical qualities of variational inference in general. In particular, there is value in showing classical qualities such as asymptotic normality to improve the applicability of modern techniques. \cite{Wang:2019:VBVM} and \cite{Wang::mis2019} provide a strong start, but there is an entire literature of just BvM-type results to adapt to VI. At the same time, as VI continues to adapt to new problems, theoretical study needs to keep pace with these developments as well. We expect these theoretical advances to be plentiful in the near future. 